# Sunday, April 6th

Related Files: 15-112 > Term Project  
Audio_040625  
Video_040625

- Refined the goal and scope of my project: to automate scoring for the KBIT-2  
  intelligence test by using OCR to extract information from standardized tables,  
  particularly Table B1.  
- Followed a systematic approach to understand the scoring logic by labeling each  
  score component (A1–F3) and tracing how each one is derived:  
    - A1–A3 represent raw input scores from the test.  
    - B1 and B2 are sums of the raw scores.  
    - C1–C6 (standard scores, confidence intervals, and percentiles) are derived from  
      Table B1.  
    - D1A and D1B are percentile-based descriptions also from the same table.  
    - F1–F3 relate to composite IQ and descriptive categories.  

[K-BIT2 Scoring](https://www.notion.so/K-BIT2-Scoring-1cd5c24faeb080ce9fe8d882825b6d2d?pvs=21)

- Identified that Table B1 (verbal and nonverbal standard score conversions) is the  
  most complex and essential for automation.  
- Documented the workflow for score calculation in a table with explanations of how  
  each value is derived.  
- Determined that future development should focus on:  
    - Extracting data programmatically from Table B1 (likely with OCR).  
    - Creating a system that auto-generates the full score profile based on raw scores.  
    - Allowing users to customize output (e.g. export as spreadsheet, select fields,  
      or print).  

### Next Steps

    - [ ]  Understand last two sections of K-BIT2  
        - [ ]  Graphical Profile  
        - [ ]  Score Comparison  
    - [ ]  Learn more about optical character recognition  
        - Which library to use? How will this interact with OpenCV?  
    - Office Hour Questions  
        1. Aim to store all the data values, store this (memoization?), and then access  
           this for each participant?  
        2. Features of the application

# Saturday, April 19th

## Brain Dump
1. Need a way to convert the PDF with all the Table 1 scans into images that can be processed
2. Basically take the scanned PDF, seperate each image, process each image through OCR
(OCR is what makes it gray scale), then use tesseract to do text extraction
3. Not entirely sure yet on how I want to store the data/what data structure I want to use
4. But data extracted should be seperated by age and month, since that's what determines the
other unique parameters of the test

## Processing Code Output
Table B.1: Verbal and Nonverbal Standard Scores, Confidence Intervals, and Percentile Ranks
| Ages 4:4-4:5 4:4-4:5
VERBAL NONVERBAL
Stand. 90% ile Raw | Stand. 90% ile Stand. 90%
Score | Score Conf. int. Rank Score | Score Conf. Int. Rank Score | Score Conf. Int. rile
108 | 160 148-165 >99.9 51 145 134-151 99.9 46 160 142-164 =k
107. | 160 148-165 >99.9 50 144 133-150 99.8 45 160 142-164 2229
106 | 160 148-165 >99.9 49 143 132-149 99.8 44 160 142-164 2008
105 | 160 148-165 >99.9 48 142. 131-148 = 99.7 43 160 449-164 2229
104 | 160 148-165 >99.9 47 141 130-147. 99.7 42 160 142-164 er
103 | 160 148-165 >99.9 46 140 129-146 99.6 41 160 142-164 5990
102 | 160 148-165 >99.9 45 139 129-145 99.5 40 160 142-164 =a58
101 | 160 148-165 399.9 a4 | 137 127-143 99 39 | 160 142-164 3993
__ 100 | 160 148-165 >999 | | 43 136 126-142 99 | _38 160__142-164 5999
99 | 160 148-165 >99.9 42 135. 125-141 99 37 | 159 141-163 S999~
98 160 148-165 >99.9 Al 133 123-139 99 36 159 141-163 3999
97 160__ 148-165 >99.9 40 132___122-138__ 98 35 158 140-162 3999
96 160 148-165 399.9 39 131 (121-138 = 98 157 139-162 999
95 160 148-165 >99.9 38 130 120-137 98 155 137-160 3999
94 160___ 148-165 >99.9 37 128 118-135 97 154 137-159 5999
93 160 148-165 >99.9 36 126 116-133 96 152 135-157 3999
92 160 148-165 >99.9 35 125. 115-132 95 150 133-155 999
91 160 148-165 >99.9 34 124 114-131 95 148 131-154 999
90 160 148-165 >99.9 EF) 122. 112-129 93 28 146 129-152 999
89 160 148-165 >99.9 32 121. 112-128 92 27 143 127-149 998
88 160 148-165 >99.9 31 119 110-126 90 26 142 126-148 997
Cy] 160 148-165 >99.9 30 117. 108-124 87 25 140 124-147 99.6
86 160 148-165 >99.9 29 116 107-123 86 24 137. 122-144 99
85 160 148-165 >99.9 28 114 105-122 82 23 135 120-1429
84 160 148-165 >99.9 27 112 103-120 79 22 132 117-139 98
83 160 148-165 >99.9 26 110 101-118 = 75 21 129 114-137 97
82 160 148-165 >99.9 25 107. 98-115 «68 20 127 113-135 9%
81 160 148-165 >99.9 106 97-114 66 124 110-132 95
80 160 148-165 >99.9 104 95-112 61 121 107-130 92
79 160 148-165 >99.9 102. 94-110 55 118 105-127 88
78 160 148-165 >99.9 99 91-107 47 115 102-124 84
77 160 148-165 >99.9 97 89-105 42 112 99-122 79
76 160 148-165 >99.9 9587-10437 108° 96-118 70
75 160 148-165 >99.9 18 92 84-101 30 103. 91-114 ‘58
74 160 148-165 >99.9 17 90 82-99 25 99 88-110 47
73 160 148-165 >99.9 16 8779-96 —:19 95 84-107__—37
72 160 148-165 >99.9 15 8 78-94 16 10 91. 81-103. 27
71 159 147-164 >99.9 14 82 75-91 12 ) 88 78-101 21
70 159 147-164 _ >99.9 13 80-73-89 9 8 84-75-97 ___—si14
7) 159 147-164 >99.9 77-70-87 6 7 80. 71-93. «9
68 158 146-163 >99.9 75 68-85 5 6 77 ~—s«68-91 6
67 158 146-163 >99.9 7366-83 4 5 73-65-87. 4
66 158 146-163 >99.9 ) 71-64-81 3 4 69. 61-84 2
65 157 146-162 >99.9 8 68 62-78 2 3 65 58-80 1
64 156 145-161 >99.9 7 66 _—-60-76 1 2 61 5477 05
63 155 144-160 >99.9 6458-74 1 1 58 52-74 (03
62 154 143-159 >99.9 62 56-72 1 0 55 49-71—Ss«C
__ 61 | 154 143-159 >99.9_ 60 5471 0.4
60 153. 142-158 >99.9 58 52-6903
59 152 141-157 >99.9 56 650-67 = 0.2
58 151 140-156__>99.9 54 48-650.
57 150 139-155 >99.9 53-47-64 0.1
56 150 139-155 >99.9
55 149 138-155 __>99.9
54 148 137-154 99.9
53 147 136-153 99.9
52 146 135-152 99.9
ae
80 Table B.1: Verbal and Nonverbal Norms, Ages 4:4~-4:5

## Redefining Pipeline
1. Have the PDF with all the Table B.1 pages
2. To use OCR, I need to convert all the PDF pages to their PNG versions -> contain this in
   processPDF()
3. Then, to extract meaningful data, I need to split these PNG images into the verbal and
   nonverbal tables
4. Finally, I need to extract the text

### Splitting Image to Extract Table Data
1. Assuming I have a PNG of the table file, I want to split it into the three tables on the
   page. I plan to do this by looking for vertical lines:

Verbal (Table 1):  
Start Line: 1, End Line: 3

Verbal (Table 2):  
Start Line: 4, End Line: 6

Nonverbal (Table 3):  
Start Line: 7, End Line: 9

2. Then, I want to find the location for each of the vertical lines  
3. Using the location for each, I want to create three new PNGs, store them in a new folder  
4. Then, I want to create an algorithm to extract the text from each of these pages. This is
   easier, because it's just recognizing the same four columns and turning it into a data
   structure I can run this on all the images for.

## Finding Location of Vertical Lines
Tutorial: https://www.youtube.com/watch?v=E_NRYxJyZlg

- Mask: Binary black-and-white images, allows us to specify which regions we're
  interested in (usually white)
- Erosion: Removing elements
- Dilation: Expanding or growing elements
- Kernels: What you specify, this is how you decide how to change the image to create the
  mask you want

This is the algorithm I want to implement based on the video:

1. For my project, to split each image, I want to make the vertical lines white (this is what
   I want information on), and the rest of the image, with the number values, black
2. This is just temporarily, because to find the split points, I'm only interested in where
   the black lines are
3. I will have to create a kernel that's a thin vertical line, because that's the element I
   want to isolate for this step
4. Use getStructuringElement, because I want a line, this will be a rectangle
5. Note on syntax: this will be getStructuringElement(element, (width, height))
6. Erode the table image (it'll just be a white screen with black lines, these black lines
   are the table lines)
7. Get the location of the black lines

Pseudocode:  
def getVerticalLinesPositions(filePath):  
    create thresholded image (inverted black and white)  
    create image with only vertical lines remaining  
    find location of each vertical line  

## Finding Contours:
Resource: https://docs.opencv.org/4.x/d4/d73/tutorial_py_contours_begin.html  
1. Use cv2.findContours(image, mode, method) to find the contours  
2. image: The binary input image  
3. mode: Contour retrieval mode, e.g., cv2.RETR_EXTERNAL (external contours only),  
   cv2.RETR_LIST (all contours), cv2.RETR_TREE (hierarchical contours).  
4. method: Contour approximation method, e.g., cv2.CHAIN_APPROX_SIMPLE (compress horizontal,
   vertical, and diagonal segments into their end points), cv2.CHAIN_APPROX_NONE (store all
   contour points)

# Sunday, April 20th

## Data Extraction for Nonverbal Table  
1. Need to preprocess image  

## Office Hours  
1. Process the image, clean the text you get / make sure all the anomalies are gone  
2. Turn each line into a row in a CSV file (each table page is its own CSV)  
3. Can call CSV whenever I need patient information  
4. Create separate folders for each stage of processing  
5. Store data as a pandas file (.csv)

# Monday, April 21st

## Data Extraction for Tables  
- What do I currently have right now? A way to extract text, but I want to create an
  algorithm to actually clean and process this so much better
- Either I can work directly on cleaning the text I already get, and go from there, or I can
  go back and do some more image processing and then extract the text
- What do I know about each table as of now? What should the final table/CSV look like?

### Verbal Table Information  
Columns: Raw Score, Stand. Score, 90% Conf. Int., % ile Rank  
Table 1:  
- First column of values is always for the raw score, goes from 108 to 52 (this does NOT
  change for any table)
- Second column of values is the standard score, starts at 160, but these values change
  depending on the table, but are always decreasing
- Third column is the 90% confidence interval, this also changes depending on age, but is
  always a range less than 100
- Fourth column is percentile rank, this also change depending on age

Table 2:  
- First column of values is always for the raw score, goes from 51 to 0 (this does NOT
  change for any table)
- The only non-numerical values that should be recognized are -, > and .

### Nonverbal Table Information  
- First column of values is always for the raw score, goes from 46 to 0 (this does NOT
  change for any table)

### CSV Table Files  
1. For verbal table, this should contain a heading row, then columns numbered from 108 to 0  
2. For nonverbal table, this should also contain a heading row, then columns numbered from 46 
to 0

### Common Errors for Validity Checks
1. All data points should start and end with digits
- Created checkDecimalPoints(line)
54 152. 141-157 >99.9

2. The percentile should be between 99.9 and 0.1, and the > should not be recognized as a
number value
57 154 143-159 399.9

3. The value of the raw score must be between the expected values, based on the rawScores
dictionary

## Progress
Produced three text extraction files from algorithm (Term Project > test_csv_files)
1. page_78_verbal1_1: Table 1, didn't add - character to expected characters, so range
values weren't correct
2. page_78_verbal1_2: Table 1, after adding - character
3. page_80_verbal1_1: Table 1, for another page

# Tuesday, April 22nd

### Common Errors for Validity Checks (Cont'd)
4. If there are three values, they often start from the standard score
160 148-165 399.9

5. If there are three hyphens, check that the first number in the range is smaller than the
second number, if not, seperate them
85 160-148-165. >99.9

## Change to Data Extraction Algorithm
- Populate the expected dictionary first, then search by keys and update

## Looping through Files
1. Eventually want to organize everything by age (therefore group things by page, instead of
table type)
2. Create a folder within the test_csv_files for each page, then populate it with the CSVs
for each table type
3. In a new function, join the verbal1.csv and verbal2.csv for each page into a new CSV,
then store this

## Progress
- Significant work on text extraction methods
- Logic to loop through folders/save final CSV files
